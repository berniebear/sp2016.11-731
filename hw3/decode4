#!/usr/bin/env python
import argparse, sys, heapq, models
import numpy as np
import pdb
from collections import namedtuple
import rangeset
#pdb.set_trace()
parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
opts = parser.parse_args()

def initialize_future_span(future_span,f):
    for span_size in xrange(len(f)):
        for start in xrange(len(f)+1):
            end = start + span_size
            if end > len(f): # out of boundry
                continue
            new_phrase = f[start:end]
            if start == end:
                future_span[(start,end)] = 0.0
            else:
                cost = -999999.0
                for prev_span_size in xrange(1,span_size):
                    # check max (pp28.)
                    middle = start + prev_span_size
                    cost = max(cost, future_span[(start, middle)] + future_span[(middle, end)])
                if new_phrase in tm:
                    for phrase in tm[new_phrase]:
                        # TM logprob
                        tm_logprob = phrase.logprob
                        # LM logprob
                        lm_logprob = 0.0
                        lm_state = lm.begin()
                        for word in phrase.english.split():
                            (lm_state, word_logprob) = lm.score(lm_state, word)
                            lm_logprob += word_logprob
                        lm_logprob += lm.end(lm_state) #if j == len(f) else 0.0
                        cost = max(cost, tm_logprob + lm_logprob)
                future_span[(start,end)] = cost

def future_estimation(future_span, spans):
    assert isinstance(spans, rangeset.RangeSet)
    l_range = list(spans)
    return sum([future_span[ra] for ra in l_range])
    #return sum( [future_span[span] for span in list(spans)])


def gen_spans(span):
    for start in xrange(span[0], span[1] + 1):
        for end in xrange(start, span[1] + 1):
            yield (start, end)


tm = models.TM(opts.tm, sys.maxint)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]
#pdb.set_trace()
#hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase')
hypothesis = namedtuple('hypothesis', 'logprob, translated_logprob, lm_state, predecessor, phrase, translated, future')
for f in input_sents:
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    initial_hypothesis = hypothesis(0.0, 0.0, lm.begin(), None, None, [],  rangeset.RangeSet(0, len(f)))
    initial_hypothesis_r = hypothesis(0.0, 0.0, lm.begin(), None, None, [], rangeset.RangeSet(0, len(f)))

    # clear future span table
    future_span = dict()
    initialize_future_span(future_span,f)

    stacks = [{} for _ in f] + [{}]
    stacks_r = [{} for _ in f] + [{}]
    #stacks[0][lm.begin()] = initial_hypothesis
    stacks[0][""] = initial_hypothesis
    stacks_r[-1][""] = initial_hypothesis_r

    for idx, stack in enumerate(stacks[:-1]):
        # extend the top s hypotheses in the current stack
        for h in heapq.nlargest(opts.s, stack.itervalues(), key=lambda h: h.logprob): # prune, default max only
            #for j in xrange(i+1,len(f)+1): # search from i-th translated word to end
            for future in h.future:
                for generated_span in gen_spans(future):
                    i,j = generated_span[0:2]
                    if f[i:j] in tm: # is a pharse
                        future_new = h.future - (i,j)
                        #print type(h.future), type(future_new)
                        future_logprob = future_estimation(future_span, future_new)

                        for phrase in tm[f[i:j]]: # find all possible phrase translation
                            # select insertion point
                            for k in range(len(h.translated)+1):
                                # words to translate  translated - inserted phrase - translated
                                toTranslate = h.translated[:k] + [phrase.english.split()] + h.translated[k:]
                                #toTranslate = [phrase.english.split()] 
                                
                                # TM log-prob
                                tm_logprob =  h.translated_logprob + phrase.logprob
                                translated_logprob = tm_logprob
                                
                                # LM log-prob
                                lm_logprob = 0.0
                                lm_state = lm.begin()
                                temp = [x for y in toTranslate for x in y]
                                for word in temp:
                                    (lm_state, word_logprob) = lm.score(lm_state, word)
                                    lm_logprob += word_logprob
                                lm_logprob += lm.end(lm_state) #if j == len(f) else 0.0
                                
                                # reordering score
                                #weight = 1.0
                                #if (h.predecessor != None):
                                #    dist = abs(h.predecessor.last_pos-k)/10
                                #    weight = np.power(alpha,-dist)
                                # to add
                                
                                           #('hypothesis', 'logprob, translated_logprob, lm_state, predecessor, phrase, translated')
                                new_hypothesis = hypothesis(tm_logprob+lm_logprob+future_logprob, translated_logprob, lm_state, h, phrase, toTranslate, future_new)
                                sent = " ".join([x for y in toTranslate for x in y])
                                if sent not in stacks[j] or stacks[j][sent].logprob < tm_logprob + lm_logprob:
                                    stacks[j][sent] = new_hypothesis
                                #if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination:
                                #    stacks[j][lm_state] = new_hypothesis 
    '''
    for i, stack_r in enumerate(reversed(stacks_r[0:])):
        i_r = len(stacks_r[0:])-1-i
        # extend the top s hypotheses in the current stack
        for h in heapq.nlargest(opts.s, stack_r.itervalues(), key=lambda h: h.logprob): # prune, default max only
            for j in reversed(xrange(0,i_r)): # search from i-th translated word to end
                if f[j:i_r] in tm: # is a pharse
                    for phrase in tm[f[j:i_r]]: # find all possible phrase translation
                        # select insertion point
                        for k in range(len(h.translated)+1):
                            # words to translate  translated - inserted phrase - translated
                            toTranslate = h.translated[:k] + [phrase.english.split()] + h.translated[k:]
                            #toTranslate = [phrase.english.split()] 
                            
                            # TM log-prob
                            tm_logprob =  h.translated_logprob + phrase.logprob
                            translated_logprob = tm_logprob

                            # LM log-prob
                            lm_logprob = 0.0
                            lm_state = lm.begin()
                            temp = [x for y in toTranslate for x in y]
                            for word in temp:
                                (lm_state, word_logprob) = lm.score(lm_state, word)
                                logprob += word_logprob
                            lm_logprob += lm.end(lm_state) #if j == len(f) else 0.0
                            
                            # reordering score
                            # to add
                            
                            new_hypothesis = hypothesis(tm_logprob + lm_logprob, translated_logprob, lm_state, h, phrase, toTranslate)
                            sent = " ".join([x for y in toTranslate for x in y])
                            if sent not in stacks_r[j] or stacks_r[j][sent].logprob < tm_logprob + lm_logprob:
                                stacks_r[j][sent] = new_hypothesis
                            #if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination
                            #    stacks[j][lm_state] = new_hypothesis 
    '''
    # find best translation by looking at the best scoring hypothesis
    # on the last stack
    winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
    #winner = max([max(stacks[-1].itervalues(), key=lambda h: h.logprob), max(stacks_r[0].itervalues(), key=lambda h: h.logprob)], key=lambda h: h.logprob)

    #pdb.set_trace()
    def extract_english_recursive(h):
        #return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)
        return " ".join([x for y in h.translated for x in y])
    print extract_english_recursive(winner)

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write('LM = %f, TM = %f, Total = %f\n' % 
            (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
